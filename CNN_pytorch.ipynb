{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "CNN_pytorch.ipynb",
      "authorship_tag": "ABX9TyOn1Tn9bnbbI656XgXziSvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kansi420/GPT_from_Scratch/blob/main/CNN_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcx5mv1kD5PN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8, 8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 10 random images of shape (1, 28, 28)\n",
        "images = torch.rand(10, 1, 28, 28)\n",
        "\n",
        "# Build 6 conv. filters\n",
        "conv_filters = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3, 3), stride=1, padding=1)\n",
        "\n",
        "# Convolve the image with the filters\n",
        "output_feature = conv_filters(images)\n",
        "print(output_feature.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW9RJ6gHEBcX",
        "outputId": "dcba7305-3fb3-45ab-8e73-c28c6eb12ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 6, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 10 random images\n",
        "images = torch.rand(10, 1, 28, 28)\n",
        "\n",
        "# Create 6 filters\n",
        "filters = torch.rand(6, 1, 3, 3)\n",
        "\n",
        "# Convolve the image with the filters\n",
        "output_feature = F.conv2d(images, filters, stride=1, padding=1)\n",
        "print(output_feature.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqt37BjlEL1O",
        "outputId": "ec39e906-4498-4ea9-be2a-67927690e856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 6, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im = torch.rand(1, 1, 6, 6)\n",
        "im"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo4TaIYGEPXN",
        "outputId": "2f0e07f9-e9e3-4043-efe9-e97a61680465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.5240, 0.8292, 0.9949, 0.1586, 0.4913, 0.4804],\n",
              "          [0.1126, 0.0686, 0.7761, 0.1324, 0.6506, 0.6817],\n",
              "          [0.4351, 0.8818, 0.7890, 0.9542, 0.4715, 0.0519],\n",
              "          [0.0281, 0.7751, 0.6171, 0.4897, 0.6183, 0.7854],\n",
              "          [0.5114, 0.2060, 0.5826, 0.9603, 0.8875, 0.2266],\n",
              "          [0.8581, 0.8772, 0.7913, 0.3733, 0.1030, 0.8552]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a pooling operator with size 2\n",
        "max_pooling = nn.MaxPool2d(2)\n",
        "\n",
        "# Apply the pooling operator\n",
        "output_feature = max_pooling(im)\n",
        "\n",
        "# Use pooling operator in the image\n",
        "output_feature_F = F.max_pool2d(im, 2)\n",
        "\n",
        "# Print the results of both cases\n",
        "print(output_feature)\n",
        "print(output_feature_F)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inZuaCl_ESjO",
        "outputId": "3c68e20e-1d8c-4e3b-cdee-aec51d548588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.8292, 0.9949, 0.6817],\n",
            "          [0.8818, 0.9542, 0.7854],\n",
            "          [0.8772, 0.9603, 0.8875]]]])\n",
            "tensor([[[[0.8292, 0.9949, 0.6817],\n",
            "          [0.8818, 0.9542, 0.7854],\n",
            "          [0.8772, 0.9603, 0.8875]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a pooling operator with size 2\n",
        "avg_pooling = nn.AvgPool2d(2)\n",
        "\n",
        "# Apply the pooling operator\n",
        "output_feature = avg_pooling(im)\n",
        "\n",
        "# Use pooling operator in the image\n",
        "output_feature_F = F.avg_pool2d(im, 2)\n",
        "\n",
        "# Print the results of both cases\n",
        "print(output_feature)\n",
        "print(output_feature_F)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h01aeU7SEabt",
        "outputId": "c81dec45-3f4e-4da3-8440-03fa162e8c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.3836, 0.5155, 0.5760],\n",
            "          [0.5300, 0.7125, 0.4817],\n",
            "          [0.6131, 0.6769, 0.5181]]]])\n",
            "tensor([[[[0.3836, 0.5155, 0.5760],\n",
            "          [0.5300, 0.7125, 0.4817],\n",
            "          [0.6131, 0.6769, 0.5181]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, num_classes=1000):\n",
        "      super(AlexNet, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "      self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
        "      self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
        "      self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "      self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "      self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "      self.fc2 = nn.Linear(4096, 4096)\n",
        "      self.fc3 = nn.Linear(4096, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.relu(self.conv1(x))\n",
        "      x = self.maxpool(x)\n",
        "      x = self.relu(self.conv2(x))\n",
        "      x = self.maxpool(x)\n",
        "      x = self.relu(self.conv3(x))\n",
        "      x = self.relu(self.conv4(x))\n",
        "      x = self.relu(self.conv5(x))\n",
        "      x = self.maxpool(x)\n",
        "      x = self.avgpool(x)\n",
        "      x = x.view(x.size(0), 256 * 6 * 6)\n",
        "      x = self.relu(self.fc1(x))\n",
        "      x = self.relu(self.fc2(x))\n",
        "      return self.fc3(x)"
      ],
      "metadata": {
        "id": "UHH_3gNzEdVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Instantiate two convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
        "\n",
        "        # Instantiate the ReLU nonlinearity\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Instantiate a max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Instantiate a fully connected layer\n",
        "        self.fc = nn.Linear(49 * 10, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply conv followed by relu, then in next line pool\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Apply conv followed by relu, then in next line pool\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Prepare the image for the fully connected layer\n",
        "        x = x.view(-1, 7 * 7 * 10)\n",
        "\n",
        "        # Apply the fully connected layer and return the result\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "BIw9nH99Eltd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Transform the data to torch tensors and normalize it\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307), (0.3081))\n",
        "])\n",
        "\n",
        "# Preparing the training and test set\n",
        "trainset = torchvision.datasets.MNIST('mnist', download=True, train=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST('mnist', download=True, train=False, transform=transform)\n",
        "\n",
        "# Prepare loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv3DLK2uEoNY",
        "outputId": "73f7a1d7-01ea-4cdc-b58d-ea9c88f5b595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 56383402.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1625349.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13913342.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21073593.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YhV4VKITEqMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute the forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Compute the loss function\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "g-4-avYWE3C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "# Iterate over the data in the test_loader\n",
        "for i, data in enumerate(test_loader):\n",
        "    # Get the image and label from data\n",
        "    image, label = data\n",
        "\n",
        "    # Make a forward pass in the net with your image\n",
        "    output = net(image)\n",
        "\n",
        "    # Argmax the results of the net\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "    if predicted == label:\n",
        "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
        "    else:\n",
        "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + str(label))\n",
        "\n",
        "    if i > 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikkjg5p8E6G7",
        "outputId": "03d85dae-b1e5-4af6-b2e5-cd9fb8a18349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yipes, your net made the right prediction tensor([7])\n",
            "Yipes, your net made the right prediction tensor([2])\n",
            "Yipes, your net made the right prediction tensor([1])\n",
            "Yipes, your net made the right prediction tensor([0])\n",
            "Yipes, your net made the right prediction tensor([4])\n",
            "Yipes, your net made the right prediction tensor([1])\n",
            "Yipes, your net made the right prediction tensor([4])\n",
            "Yipes, your net made the right prediction tensor([9])\n",
            "Yipes, your net made the right prediction tensor([5])\n",
            "Yipes, your net made the right prediction tensor([9])\n",
            "Yipes, your net made the right prediction tensor([0])\n",
            "Yipes, your net made the right prediction tensor([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NShiHMVYE8LJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}